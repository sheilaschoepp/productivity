---
title: Interactively Shaping Agents via Human Reinforcement
author(s):
  - W. Bradley Knox
  - Peter Stone
year: 2009
doi: https://doi.org/10.1145/1597735.1597738
summary: A TAMER agent is trained using human reinforcement in an MDP\R environment and is tested in both Tetris and Mountain Car.
keyword(s):
  - human-agent interaction
  - shaping
  - sequential decision-making
  - human teachers
  - learning agents
"% read": "100"
quality rating: "3"
relevance rating: "0"
created: 2023-09-11
accessed: 2023-12-23
tags:
  - HumanInTheLoopLearning
  - ReinforcementLearning
related:
  - "[[Learning from Advice]]"
  - "[[Learning from Demonstration]]"
  - "[[Learning from Reinforcement (Shaping)]]"
index:
---
# [[Knox 2009.pdf|Interactively Shaping Agents via Human Reinforcement]]

>[!summary]
>Training an Agent Manually via Evaluative Reinforcement (TAMER).

>[!central-problem] Central Problem
>In the real world, it is important to be able to learn a task quickly, with minimal learning trials (i.e., low sample complexity).

>[!proposed-solution] Proposed Solution
>With [[Learning from Reinforcement (Shaping)]], a human with task knowledge guides agent learning by providing the agent with a scalar reinforcement signal (positive or negative) in response to the agent's action choice (or short sequence of action choices). The agent ultimately learns a policy that maximizes the expected reinforcement from the human.
>
>Benefits of including a human in the learning process:
>* Reduces sample complexity (e.g., number of episodes) for learning a task.
>* Agent can learn a task without a pre-defined reward function (MDP\R).
>* Users without technical skills can teach the agent.
>* Agents can learn more complex tasks than autonomous learning allows.
>* Resolves the problem of [[Credit Assignment]] in RL because the human is giving feedback over short intervals.
>
>Limitations of including a human in the learning process:
>* The agent's skill is restricted to be less than or equal to that of the human teacher.

>[!methodology] Methodology
>* Modelling the human's reinforcement function is a supervised learning problem.
>* The human's reinforcement function (i.e., the target function) may be non-stationary.
>* There may be unseen states and/or state-action pairs (i.e., model needs to be generalizable).
>* TAMER allows for the designer to specify how the state-action space will be explored; however, the authors claim greedy action selection provides sufficient exploration for many tasks, as poor actions are negatively reinforced.
>* Model is not updated when human reinforcement is zero (i.e., $h=0$); it is possible that the human stopped interacting with the system for some reason.
>
>How does TAMER differ from RL?
>* The human reinforcement function $H$ is not a replacement for the MDP reward function $R$. 
>* In RL, the goal is to maximize the expected discounted return (i.e., expected discounted cumulative sum of rewards). TAMER aims to maximize the expected reinforcement from the human, which is not obviously farsighted and not cumulative.  However, although not obviously farsighted, human reinforcement may be farsighted by nature, as humans may be able to better perceive how taking one action can lead to the ultimate goal.
>* Use MDP\R's since $H$ defines a policy of behaviour, whereas the tradition RL MDP reward function $R$ also defines a policy.
>
>Credit Assignment
>*  #TODO

>[!results] Results
>**Table 1**
>* TAMER in a slow game, allowing for the human teacher to reinforce each action.
>* TAMER outperforms the only other algorithm that has incremental updates. All other algorithms need to wait for a batch of data to be collected prior to performing an update.
>* TAMER achieves a performance that pales in comparison to most of the other algorithms. However, these algorithms are unstable and often degrade in performance, reaching a final performance that is less than half of the peak performance (i.e., it unlearns). TAMER, in contrast, never unlearns.
>
>![[Screenshot 2023-09-11 at 8.25.16 PM.png]]
>
>**Figure 4**
>* The background of the teachers is not significant to the performance of the TAMER algorithm. In game 9, those with no technical background achieved a higher performance than those with an AI background.
>
>![[Screenshot 2023-09-11 at 8.25.35 PM.png]]
>
>**Figure 5**
>* TAMER in a fast environment, preventing the human teacher from reinforcing each action. This setup uses Algorithm 2 (credit assignment).
>* TAMER outperforms SARSA-20 in the first episodes, but later matches its performance. SARSA-3 is unstable and unlearns.
>* The best trainers for TAMER outperform SARSA-20, while the worst trainers underperform marginally compared to SARSA-20.
>
>![[Screenshot 2023-09-11 at 8.25.58 PM.png]]
>
>**Figure 6**
>* TAMER has less sample complexity (i.e., the mean cumulative environmental reward at each episode is less than that of the autonomous algorithms). Note: -1 reward is collected at each time step.
>
>![[Screenshot 2023-09-11 at 8.25.51 PM.png]]

>[!implications-and-future-work] Implications and Future Work
>* TAMER performs better than autonomous learning algorithms in the short term (i.e., when most algorithms have not even started learning), but its performance is far exceeded in the long term. Is this a convincing reason to use it?
>* How would TAMER work in learning tasks where a human can provide reinforcement only across larger periods of time (i.e., more state-action pairs are visited between evaluations)?  For example, in robotics, you can't see what the arm is doing by looking at the micro-movements. You need to wait and observe a series of movements to get an idea of what the robot arm is doing.


## Algorithms

![[Screenshot 2023-09-11 at 5.13.31 PM.png|450]]
![[Screenshot 2023-09-11 at 7.57.36 PM.png|450]]

## Questions and Comments

* [ ] Is that author assuming that the advice function is stationary?  It would make sense for humans to have some non-stationarity in their rewards.
* [ ] (location unknown) Is the agent's skill limited to that of the human?  Or can the skill of the agent surpass that of the human?
* [ ] (3.1 second paragraph) The algorithm must be generalizable to all unseen samples (i.e., to states and POSSIBLY actions).
* [ ] (3.1 last paragraph) Were the episodes of fixed length?
* [ ] (3.1.1) Greedy action selection does not seem like it would necessarily allow for sufficient exploration. It could easily be the case that all actions are good, but the first one explored is attributed to be good and thereafter that action is always taken. Meanwhile there may be another one that is slightly better.
* [ ] (3.2 second paragraph) How is the feature vector and the error used to update the model?
* [ ] Since TAMER is more near-sighted (i.e., maximizing the expected reinforcement from the human teacher in the short term), does it truly find the best policy?  This raises the issue of whether the policy found is limited by the human teacher.
* [ ] Table 1: Maybe I am not understanding this table correctly, but I feel conflicted about these results. First of all, if we look at the mean lines cleared at game 3 column, most of the algorithms have not yet started learning since they rely on batch updates, so there is no true comparison on how much better TAMER is in comparison. Then, when we look at the at Peak column, we see that TAMER's peak is 65.89, which I interpret as its lifetime peak. So yes, it speeds up learning initially, but it is severely restricted in peak performance. Maybe this limitation is the human's ability to see a solution?
* [ ] Safety!  It would be interesting to observe how the agents behave when they are trained by a human vs when they are autonomously learning. Are there less erratic decisions/actions being made by the human?  There are other factors in the real world to consider than just how an agent behaves (as pointed out by Matt's paper from last week).
* [ ] Figure 6: Sample Complexity. The results don't convince me that TAMER is significantly better. There is a lot of work that goes into human input to gain only marginally better results.

## Discussion

* Have a human label the state-action they are rewarding to learn the human's gamma function.
* You could also learn the human's reaction time (e.g., press the button when you see the red ball) but this may be their minimum reaction time. This is a potential project??
* They had people come in and practice, then they gave them the reward function, then they evaluated their work.
* Think about what it means to not be able to give 0 rewards. 0 reward would represent that the action was neither good or bad.
* The fact that they gave the humans reward to guide them on how to reinforce behaviour contradicts the statement "any human can teach the agent"
* The authors get away with only looking at immediate reward and not cumulative reward. They assume that humans can do a intuitive look ahead analysis. R is the immediate reward. Potentially, a layperson would have much less of an idea of the "think ahead rewards" than an expert would.
* How far can people look ahead and can they look ahead?  Potential project?
* Human subject study
	* Participants were not told about agent's features and the algorithm
	* Participants
		* 9 people for Tetris
		* 19 people for Mountain Car
		* No info about age, video game experience, gender, etc. - they could have collected this info to learn something interesting
	* Runs
		* Tetris: human trainers practiced for 2 runs, data from 3rd
		* Mountain car: practice 1 run, data from 2 and 3
		* Confusing how this is related to number of episodes/games
	* Qualitative analysis
		* It would have been nice to show the qualitative differences between the human guided policy and a policy learned autonomously
