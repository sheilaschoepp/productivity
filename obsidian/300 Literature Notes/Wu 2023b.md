---
title: "SPRING: Studying the Paper and Reasoning to Play Games"
author(s):
  - Yue Wu
  - Shrimai Prabhumoye
  - So Yeon Min
  - Yonatan Bisk
  - Ruslan Salakhutdinov
  - Amos Azaria
  - Tom Mitchell
  - Yuanzhi Li
publication: NeurIPS
year: "2023"
doi: 
summary: This paper explores the use of LLMs for understanding game manuals and applying this understanding to play games effectively. Through a process termed SPRING, it demonstrates how extracting knowledge from text can inform game strategy and decision-making.
keyword(s):
  - large language models
  - open-world games
  - reinforcement learning
read: true
quality: 5
relevance: 4
created: 2024-02-29
accessed: 2024-03-13
tags:
  - ReinforcementLearning
  - LargeLanguageModels
related: 
index: "[[Large Language Models Index]]"
---

# [[Wu 2023b.pdf|SPRING: Studying the Paper and Reasoning to Play Games]]

>[!reading-progress] Reading Progress
>- [x] Abstract
>- [x] Introduction
>- [x] Background
>- [x] Methods
>- [x] Results
>- [x] Discussion
>- [x] Conclusion
>- [x] Images
>- [x] Mathematical Formulas

>[!summary]
>This paper introduces SPRING, an approach that leverages a Large Language Model (LLM) to play an open-world survival game, namely Crafter, by reading and reasoning from the game's original academic paper. The system is designed to overcome the challenges of high sample complexity in RL. The method employs a Directed Acyclic Graph (DAG) to guide the LLM's reasoning process, leading to the identification of optimal actions in the game environment without traditional RL training.

>[!central-problem] Central Problem
>Open-world survival games require multi-tasking, deep exploration, and goal prioritization. Traditional RL approaches struggle with high sample complexity and limited effectiveness in these complex environments.

>[!proposed-solution] Proposed Solution
>The solution proposed is SPRING, a system that utilizes the LaTeX source of an open-world survival game's academic paper to provide context to an LLM. This system employs a DAG with game-related questions as nodes to guide the LLM through reasoning processes, enabling it to play the game effectively without traditional RL training.

>[!methodology]
>The methodology involves prompting the LLM with the LaTeX source as context and the agent's current observation description. Through the DAG, the system computes LLM responses for each node in topological order, directly translating the final node's LLM answer into actions in the environment.
>
>![[Wu 2023b 1.png]]
>
>![[Wu 2023b 2.png]]
>
>![[Wu 2023b 3.png]]

>[!results]
>Experiments demonstrate that SPRING, especially when combined with GPT-4, outperforms state-of-the-art RL baselines trained for 1 million steps in the Crafter environment. This is achieved without any training on the LLM's part.
>
>![[Wu 2023b 4.png]]
>
>![[Wu 2023b 5.png]]
>
>![[Wu 2023b 6.png]]

>[!implications-and-future-work] Implications and Future Work
>The authors highlight several future directions for improving and extending their work with SPRING:
>
>* Object Recognition and Grounding: Addressing the challenge of integrating object recognition with LLMs to enhance interaction with the environment. This involves leveraging advancements in visual-language models to create reliable and generalizable solutions for visual-language understanding.
>* Policy Informed by Natural Language Instructions: Enhancing LLMs to utilize natural language instructions more effectively, potentially for generating auxiliary rewards in sparse reward environments, and moving beyond instruction following to learn from unstructured natural language for better generalization and reasoning in games.
>* Efficient Planning and Fine-Grained Control: Combining the benefits of RL and LLMs for efficient planning and fine-grained control in game environments. This includes leveraging the LLM's ability to process and reason with external knowledge sources, like the LaTeX source of academic papers, to improve decision-making in games without the need for explicit reward signals.
>* Exploration of Open-World Games: The work opens up prospects for applying similar frameworks to more complex and open-world games, where the integration of reading comprehension, reasoning, and action selection can be further tested and refined.

>[!related-ln] Related

>[!research-questions]  Research Questions
>

>[!my-notes] Notes
>SPRING was not trained with reward, but reward is included for comparison.
>
>LLMs cannot update their knowledge base through interaction.
>
>Questions in DAG were created by domain experts (humans).
>
>LLM was zero-shot (but was pre-trained with billions of documents).
>
>LLMs don't do as well with lists (compared to DAG) because they cannot remember earlier elements as well as later elements.
>
>Made a comparison to random.
